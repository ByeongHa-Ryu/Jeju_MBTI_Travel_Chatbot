{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 크롤링 이후 리뷰 데이터에서 불용어, 특수문자 등 전처리\n",
    "## 2. 전처리 된 리뷰 데이터에서 형태소 추출\n",
    "## 3. 형태소 추출된 데이터에서 TF-IDF, KrWordRank로 키워드 20개 추출\n",
    "## 4. 형태소 추출된 데이터에서 Gemini 이용 키워드 20개 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyword Extraction by TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 팀원들의 리뷰 데이터 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./review.json', 'r') as f:\n",
    "    review = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('keywords_yj.json', 'r', encoding='utf-8') as file:\n",
    "    review_yj = json.load(file)\n",
    "with open('mct_keyword_ms.json', 'r', encoding='utf-8') as file:\n",
    "    review_ms = json.load(file)\n",
    "with open('gemini_keywords_colab.json', 'r', encoding='utf-8') as file:\n",
    "    review_yj2 = json.load(file)\n",
    "review_ms = pd.DataFrame(columns=['MCT_NM', 'keyword'], data=review_ms.items())\n",
    "review_yj = pd.DataFrame(columns=['MCT_NM', 'keyword'], data=review_yj.items())\n",
    "review_yj2 = pd.DataFrame(columns=['MCT_NM', 'keyword'], data=review_yj2.items())\n",
    "review = pd.concat([review_ms, review_yj, review_yj2])\n",
    "review.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = review.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['visit_mean'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCT_NM</th>\n",
       "      <th>review</th>\n",
       "      <th>visit_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>탕탕이2호점</td>\n",
       "      <td>[맛있어요, 맛있어요, 비싸지만 아주 맛있게 먹었습니다!!!, 재료가 신선하고 푸짐...</td>\n",
       "      <td>1.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>뚱돼지</td>\n",
       "      <td>[이런글 쓰고 싶진 않은데, 죄송하지만 살면서 먹은 고기중에 제일 별로였습니다......</td>\n",
       "      <td>1.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>송이축산정육식당</td>\n",
       "      <td>[점심시간 메뉴를 고민하다가 최근 신메뉴가 나왔다고해서 먹으러 왔습니다. 원래 돼기...</td>\n",
       "      <td>1.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24시뼈다귀탕 신서귀점</td>\n",
       "      <td>[오랫만에 다시 찾은 곳💗\\n여전히 맛있고 친절하시고~\\n술맛도 너무 좋아요...!...</td>\n",
       "      <td>1.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>명당양과노형점</td>\n",
       "      <td>[옛 기억의 동네 빵집, 동네 빵집이에요, 맛있어요, 좋아요, 맛있어요, 좋아요, ...</td>\n",
       "      <td>2.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>큰가름본점</td>\n",
       "      <td>[ㅇㅇ, 감자탕 야채가 좀 부족한듯했어요., 맛있어서 가는곳인데 새해들어 볶음밥가격...</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>뚜레쥬르 제주아라아이파크점</td>\n",
       "      <td>[직원분이 친절하셨어요 빵 잘 구입했습니다, 아르바이트 하시는 직원분이 정말 친절하...</td>\n",
       "      <td>2.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>마라힐링</td>\n",
       "      <td>[3번째방문인데 리뷰는 첨쓰네용\\n넘맛있어요ㅎ\\n꿔바로우소스는많이셔서 주의해야하지만...</td>\n",
       "      <td>1.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>곽지해녀의집</td>\n",
       "      <td>[1년전인가 2년전인가 우연히 방문했다가 그동안 먹어봤던 물회와는 전혀다른 담백깔끔...</td>\n",
       "      <td>1.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>파페로(papero)</td>\n",
       "      <td>[오늘의 피자(참치 페스토 제노베제), 카르파치오, 라구비앙코 주문했습니다!! 예약...</td>\n",
       "      <td>1.080000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             MCT_NM                                             review  \\\n",
       "0            탕탕이2호점  [맛있어요, 맛있어요, 비싸지만 아주 맛있게 먹었습니다!!!, 재료가 신선하고 푸짐...   \n",
       "1               뚱돼지  [이런글 쓰고 싶진 않은데, 죄송하지만 살면서 먹은 고기중에 제일 별로였습니다......   \n",
       "2          송이축산정육식당  [점심시간 메뉴를 고민하다가 최근 신메뉴가 나왔다고해서 먹으러 왔습니다. 원래 돼기...   \n",
       "3      24시뼈다귀탕 신서귀점  [오랫만에 다시 찾은 곳💗\\n여전히 맛있고 친절하시고~\\n술맛도 너무 좋아요...!...   \n",
       "4           명당양과노형점  [옛 기억의 동네 빵집, 동네 빵집이에요, 맛있어요, 좋아요, 맛있어요, 좋아요, ...   \n",
       "..              ...                                                ...   \n",
       "165           큰가름본점  [ㅇㅇ, 감자탕 야채가 좀 부족한듯했어요., 맛있어서 가는곳인데 새해들어 볶음밥가격...   \n",
       "166  뚜레쥬르 제주아라아이파크점  [직원분이 친절하셨어요 빵 잘 구입했습니다, 아르바이트 하시는 직원분이 정말 친절하...   \n",
       "167            마라힐링  [3번째방문인데 리뷰는 첨쓰네용\\n넘맛있어요ㅎ\\n꿔바로우소스는많이셔서 주의해야하지만...   \n",
       "168          곽지해녀의집  [1년전인가 2년전인가 우연히 방문했다가 그동안 먹어봤던 물회와는 전혀다른 담백깔끔...   \n",
       "169     파페로(papero)  [오늘의 피자(참치 페스토 제노베제), 카르파치오, 라구비앙코 주문했습니다!! 예약...   \n",
       "\n",
       "     visit_mean  \n",
       "0      1.047619  \n",
       "1      1.064516  \n",
       "2      1.960000  \n",
       "3      1.633333  \n",
       "4      2.680000  \n",
       "..          ...  \n",
       "165    1.250000  \n",
       "166    2.580000  \n",
       "167    1.407407  \n",
       "168    1.020000  \n",
       "169    1.080000  \n",
       "\n",
       "[170 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "def remove_non_korean(text):\n",
    "    \"\"\"리뷰에서 한글을 제외한 모든 문자 제거\"\"\"\n",
    "    return re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣\\s]\", \"\", text)\n",
    "\n",
    "\n",
    "def load_stopwords(filepath=\"k_stopword.txt\"):\n",
    "    \"\"\"불용어 txt 파일을 읽어 불용어 리스트 반환\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "\n",
    "def okt_tokenize(text):\n",
    "    \"\"\"Okt를 이용한 형태소 분석 및 불필요 품사 제거\"\"\"\n",
    "    # 형태소 분석 및 품사 태깅\n",
    "    words = okt.pos(text, stem=True)  # 어간 추출\n",
    "    \n",
    "    # 필요한 품사만 필터링 (명사, 동사, 형용사, 부사)\n",
    "    selected_words = [\n",
    "        word for word, pos in words if pos in ['Noun', 'Verb', 'Adjective', 'Adverb']\n",
    "    ]\n",
    "    \n",
    "    # 불용어 제거\n",
    "    filtered_words = [word for word in selected_words if word not in stopwords]\n",
    "    \n",
    "    # 최종 단어들을 문자열로 반환\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def preprocess_review(text):\n",
    "    \"\"\"리뷰 텍스트를 전처리하고 형태소 분석한 최종 결과 반환\"\"\"\n",
    "    text = remove_non_korean(text)\n",
    "    processed_text = okt_tokenize(text)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def preprocess_review_list(text_list):\n",
    "    \"\"\"리뷰 텍스트를 전처리하고 형태소 분석한 최종 결과 반환\"\"\"\n",
    "    preprocessed_text_list = []\n",
    "    for text in text_list:\n",
    "        text = remove_non_korean(text)\n",
    "        processed_text = okt_tokenize(text)\n",
    "        preprocessed_text_list.append(processed_text) \n",
    "\n",
    "    #text = remove_non_korean(text)\n",
    "    #processed_text = okt_tokenize(text)\n",
    "\n",
    "    preprocessed_text = \" \".join(preprocessed_text_list)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 키워드 추출 함수\n",
    "def extract_keywords_tfidf(corpus, top_n):\n",
    "    \"\"\"TF-IDF를 사용해 각 문서의 상위 N개 키워드 추출\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    keywords = []\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        row = X[i].toarray().flatten()\n",
    "        top_indices = row.argsort()[-top_n:][::-1]\n",
    "        top_keywords = [features[idx] for idx in top_indices]\n",
    "        keywords.append(top_keywords)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_tag'] = df['keyword'].apply(preprocess_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이모티콘만 있는 경우 pos tag 값이 비어있으므로 제거\n",
    "\n",
    "df = df[df['pos_tag'].apply(lambda x: len(x)>=10)]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:00<00:00, 8400.84it/s]\n",
      "/var/folders/y3/l8bvkgx53cd0nbrk2b450t7m0000gn/T/ipykernel_10937/967659636.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tf_idf_keywords'] = extract_keywords_tfidf(df['pos_tag'].tolist(),20)\n"
     ]
    }
   ],
   "source": [
    "# pos_tag 열에서 키워드 추출\n",
    "df['tf_idf_keywords'] = extract_keywords_tfidf(df['pos_tag'].tolist(),20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kr wordrank 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install krwordrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "def krword(texts):\n",
    "    #for texts in text_list:\n",
    "    min_count = 1   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
    "    max_length = 10 # 단어의 최대 길이\n",
    "    wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "    beta = 0.85    # PageRank의 decaying factor beta\n",
    "    max_iter = 20\n",
    "    t_list = []\n",
    "    for text in texts:\n",
    "          kr_text = remove_non_korean(text)\n",
    "          processed_text = okt_tokenize(kr_text)\n",
    "          t_list.append(processed_text)\n",
    "    #texts = split_noun_sentences(text)\n",
    "    keywords, rank, graph = wordrank_extractor.extract(t_list, beta, max_iter)\n",
    "    output = []\n",
    "    num = 0\n",
    "    for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True):\n",
    "            #print('%8s:\\\\t%.4f' % (word, r))\n",
    "            output.append(word)\n",
    "            num += 1\n",
    "            if num == 21:\n",
    "                  break\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 20/65 [00:00<00:00, 466033.78it/s]\n",
      "  8%|▊         | 20/258 [00:00<00:00, 555536.95it/s]\n",
      " 11%|█         | 20/190 [00:00<00:00, 762600.73it/s]\n",
      "  9%|▊         | 20/231 [00:00<00:00, 687590.82it/s]\n",
      " 34%|███▍      | 20/58 [00:00<00:00, 397564.36it/s]\n",
      "  9%|▉         | 20/221 [00:00<00:00, 621378.37it/s]\n",
      "  4%|▍         | 20/478 [00:00<00:00, 607870.14it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 655360.00it/s]\n",
      " 80%|████████  | 20/25 [00:00<00:00, 716975.04it/s]\n",
      "  7%|▋         | 20/271 [00:00<00:00, 570653.61it/s]\n",
      "  9%|▊         | 20/234 [00:00<00:00, 527585.41it/s]\n",
      "  8%|▊         | 20/253 [00:00<00:00, 762600.73it/s]\n",
      " 11%|█▏        | 20/177 [00:00<00:00, 693273.39it/s]\n",
      "  5%|▍         | 20/431 [00:00<00:00, 645277.54it/s]\n",
      "  6%|▋         | 20/314 [00:00<00:00, 273244.56it/s]\n",
      "  6%|▌         | 20/339 [00:00<00:00, 687590.82it/s]\n",
      "  8%|▊         | 20/236 [00:00<00:00, 693273.39it/s]\n",
      "  7%|▋         | 20/272 [00:00<00:00, 660520.31it/s]\n",
      " 26%|██▌       | 20/77 [00:00<00:00, 693273.39it/s]\n",
      "  5%|▍         | 20/425 [00:00<00:00, 735842.81it/s]\n",
      "  9%|▉         | 20/222 [00:00<00:00, 626015.52it/s]\n",
      " 21%|██        | 20/97 [00:00<00:00, 590747.04it/s]\n",
      "  7%|▋         | 20/304 [00:00<00:00, 586615.94it/s]\n",
      " 13%|█▎        | 20/151 [00:00<00:00, 665762.54it/s]\n",
      "  5%|▍         | 20/411 [00:00<00:00, 742354.69it/s]\n",
      " 21%|██        | 20/96 [00:00<00:00, 607870.14it/s]\n",
      " 38%|███▊      | 20/53 [00:00<00:00, 665762.54it/s]\n",
      " 15%|█▍        | 20/137 [00:00<00:00, 555536.95it/s]\n",
      "  5%|▌         | 20/391 [00:00<00:00, 607870.14it/s]\n",
      "  9%|▉         | 20/227 [00:00<00:00, 682000.65it/s]\n",
      "  6%|▌         | 20/353 [00:00<00:00, 716975.04it/s]\n",
      "  5%|▍         | 20/441 [00:00<00:00, 665762.54it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 115591.06it/s]\n",
      "  5%|▌         | 20/373 [00:00<00:00, 716975.04it/s]\n",
      " 40%|████      | 20/50 [00:00<00:00, 716975.04it/s]\n",
      " 22%|██▏       | 20/89 [00:00<00:00, 660520.31it/s]\n",
      "  6%|▌         | 20/341 [00:00<00:00, 710898.98it/s]\n",
      " 17%|█▋        | 20/115 [00:00<00:00, 645277.54it/s]\n",
      "  6%|▌         | 20/323 [00:00<00:00, 660520.31it/s]\n",
      "  7%|▋         | 20/275 [00:00<00:00, 645277.54it/s]\n",
      " 13%|█▎        | 20/150 [00:00<00:00, 621378.37it/s]\n",
      " 10%|█         | 20/200 [00:00<00:00, 626015.52it/s]\n",
      " 40%|████      | 20/50 [00:00<00:00, 716975.04it/s]\n",
      "  8%|▊         | 20/259 [00:00<00:00, 603496.98it/s]\n",
      "  9%|▊         | 20/232 [00:00<00:00, 693273.39it/s]\n",
      "  7%|▋         | 20/304 [00:00<00:00, 603496.98it/s]\n",
      "  7%|▋         | 20/282 [00:00<00:00, 710898.98it/s]\n",
      "  6%|▌         | 20/337 [00:00<00:00, 607870.14it/s]\n",
      "  6%|▌         | 20/346 [00:00<00:00, 570653.61it/s]\n",
      "  8%|▊         | 20/240 [00:00<00:00, 735842.81it/s]\n",
      "  8%|▊         | 20/257 [00:00<00:00, 645277.54it/s]\n",
      "  8%|▊         | 20/259 [00:00<00:00, 645277.54it/s]\n",
      "  7%|▋         | 20/273 [00:00<00:00, 645277.54it/s]\n",
      "  5%|▌         | 20/395 [00:00<00:00, 762600.73it/s]\n",
      "  6%|▋         | 20/318 [00:00<00:00, 607870.14it/s]\n",
      " 24%|██▎       | 20/85 [00:00<00:00, 687590.82it/s]\n",
      " 26%|██▌       | 20/77 [00:00<00:00, 607870.14it/s]\n",
      "  7%|▋         | 20/276 [00:00<00:00, 665762.54it/s]\n",
      "  6%|▌         | 20/337 [00:00<00:00, 645277.54it/s]\n",
      "  6%|▌         | 20/347 [00:00<00:00, 607870.14it/s]\n",
      " 11%|█         | 20/188 [00:00<00:00, 586615.94it/s]\n",
      "  5%|▌         | 20/388 [00:00<00:00, 798915.05it/s]\n",
      " 16%|█▌        | 20/124 [00:00<00:00, 687590.82it/s]\n",
      "  6%|▌         | 20/321 [00:00<00:00, 671088.64it/s]\n",
      " 11%|█▏        | 20/175 [00:00<00:00, 660520.31it/s]\n",
      "  7%|▋         | 20/281 [00:00<00:00, 607870.14it/s]\n",
      "  8%|▊         | 20/253 [00:00<00:00, 645277.54it/s]\n",
      " 11%|█         | 20/183 [00:00<00:00, 742354.69it/s]\n",
      "  6%|▌         | 20/335 [00:00<00:00, 716975.04it/s]\n",
      "  6%|▌         | 20/333 [00:00<00:00, 953250.91it/s]\n",
      "  8%|▊         | 20/256 [00:00<00:00, 603496.98it/s]\n",
      " 13%|█▎        | 20/151 [00:00<00:00, 514638.53it/s]\n",
      "  7%|▋         | 20/293 [00:00<00:00, 586615.94it/s]\n",
      "  6%|▋         | 20/316 [00:00<00:00, 586615.94it/s]\n",
      " 14%|█▎        | 20/147 [00:00<00:00, 514638.53it/s]\n",
      "  6%|▌         | 20/359 [00:00<00:00, 748982.86it/s]\n",
      "  5%|▍         | 20/402 [00:00<00:00, 665762.54it/s]\n",
      "  8%|▊         | 20/256 [00:00<00:00, 769597.06it/s]\n",
      "  5%|▍         | 20/412 [00:00<00:00, 687590.82it/s]\n",
      "  4%|▍         | 20/449 [00:00<00:00, 603496.98it/s]\n",
      "  6%|▌         | 20/347 [00:00<00:00, 650279.69it/s]\n",
      " 22%|██▏       | 20/93 [00:00<00:00, 326404.98it/s]\n",
      "  4%|▎         | 20/560 [00:00<00:00, 716975.04it/s]\n",
      "  4%|▎         | 20/539 [00:00<00:00, 527585.41it/s]\n",
      "  5%|▌         | 20/371 [00:00<00:00, 665762.54it/s]\n",
      " 11%|█         | 20/181 [00:00<00:00, 776722.96it/s]\n",
      " 12%|█▏        | 20/162 [00:00<00:00, 514638.53it/s]\n",
      "  8%|▊         | 20/251 [00:00<00:00, 687590.82it/s]\n",
      " 43%|████▎     | 20/46 [00:00<00:00, 665762.54it/s]\n",
      "  5%|▍         | 20/407 [00:00<00:00, 590747.04it/s]\n",
      "  9%|▉         | 20/217 [00:00<00:00, 555536.95it/s]\n",
      " 67%|██████▋   | 20/30 [00:00<00:00, 665762.54it/s]\n",
      " 17%|█▋        | 20/121 [00:00<00:00, 612307.15it/s]\n",
      " 10%|▉         | 20/204 [00:00<00:00, 621378.37it/s]\n",
      "  7%|▋         | 20/299 [00:00<00:00, 514638.53it/s]\n",
      " 10%|▉         | 20/210 [00:00<00:00, 716975.04it/s]\n",
      "  5%|▍         | 20/415 [00:00<00:00, 742354.69it/s]\n",
      "  5%|▌         | 20/396 [00:00<00:00, 716975.04it/s]\n",
      " 13%|█▎        | 20/152 [00:00<00:00, 555536.95it/s]\n",
      "  5%|▍         | 20/434 [00:00<00:00, 769597.06it/s]\n",
      "  4%|▍         | 20/461 [00:00<00:00, 645277.54it/s]\n",
      " 11%|█         | 20/188 [00:00<00:00, 570653.61it/s]\n",
      "  4%|▎         | 20/563 [00:00<00:00, 735842.81it/s]\n",
      "  6%|▌         | 20/350 [00:00<00:00, 586615.94it/s]\n",
      " 19%|█▊        | 20/107 [00:00<00:00, 660520.31it/s]\n",
      " 15%|█▍        | 20/134 [00:00<00:00, 140748.46it/s]\n",
      " 10%|█         | 20/192 [00:00<00:00, 665762.54it/s]\n",
      " 17%|█▋        | 20/121 [00:00<00:00, 574562.19it/s]\n",
      "  6%|▋         | 20/313 [00:00<00:00, 645277.54it/s]\n",
      "  8%|▊         | 20/254 [00:00<00:00, 645277.54it/s]\n",
      "  6%|▌         | 20/327 [00:00<00:00, 645277.54it/s]\n",
      " 18%|█▊        | 20/112 [00:00<00:00, 838860.80it/s]\n",
      " 14%|█▎        | 20/147 [00:00<00:00, 911805.22it/s]\n",
      " 29%|██▉       | 20/69 [00:00<00:00, 806596.92it/s]\n",
      " 31%|███▏      | 20/64 [00:00<00:00, 735842.81it/s]\n",
      "  6%|▌         | 20/327 [00:00<00:00, 645277.54it/s]\n",
      "  5%|▍         | 20/418 [00:00<00:00, 704925.04it/s]\n",
      "  5%|▌         | 20/374 [00:00<00:00, 735842.81it/s]\n",
      " 13%|█▎        | 20/158 [00:00<00:00, 115545.56it/s]\n",
      " 12%|█▏        | 20/169 [00:00<00:00, 776722.96it/s]\n",
      "  9%|▉         | 20/227 [00:00<00:00, 645277.54it/s]\n",
      "  7%|▋         | 20/298 [00:00<00:00, 838860.80it/s]\n",
      " 16%|█▋        | 20/122 [00:00<00:00, 603496.98it/s]\n",
      "  6%|▌         | 20/333 [00:00<00:00, 716975.04it/s]\n",
      " 13%|█▎        | 20/154 [00:00<00:00, 665762.54it/s]\n",
      "  4%|▎         | 20/540 [00:00<00:00, 671088.64it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 873813.33it/s]\n",
      "  5%|▍         | 20/413 [00:00<00:00, 864804.95it/s]\n",
      " 15%|█▌        | 20/132 [00:00<00:00, 621378.37it/s]\n",
      "  8%|▊         | 20/266 [00:00<00:00, 769597.06it/s]\n",
      " 11%|█▏        | 20/177 [00:00<00:00, 687590.82it/s]\n",
      "  5%|▌         | 20/394 [00:00<00:00, 660520.31it/s]\n",
      " 11%|█         | 20/184 [00:00<00:00, 555536.95it/s]\n",
      "  7%|▋         | 20/286 [00:00<00:00, 607870.14it/s]\n",
      "  6%|▌         | 20/360 [00:00<00:00, 665762.54it/s]\n",
      "  5%|▌         | 20/371 [00:00<00:00, 687590.82it/s]\n",
      "  6%|▌         | 20/345 [00:00<00:00, 665762.54it/s]\n",
      "  6%|▌         | 20/346 [00:00<00:00, 665762.54it/s]\n",
      "  6%|▋         | 20/318 [00:00<00:00, 607870.14it/s]\n",
      "  7%|▋         | 20/298 [00:00<00:00, 645277.54it/s]\n",
      " 12%|█▏        | 20/164 [00:00<00:00, 742354.69it/s]\n",
      "  9%|▉         | 20/228 [00:00<00:00, 527585.41it/s]\n",
      "  5%|▌         | 20/389 [00:00<00:00, 621378.37it/s]\n",
      "  8%|▊         | 20/255 [00:00<00:00, 716975.04it/s]\n",
      "  7%|▋         | 20/279 [00:00<00:00, 769597.06it/s]\n",
      "  4%|▍         | 20/460 [00:00<00:00, 665762.54it/s]\n",
      "  9%|▉         | 20/224 [00:00<00:00, 570653.61it/s]\n",
      " 10%|█         | 20/197 [00:00<00:00, 586615.94it/s]\n",
      "  7%|▋         | 20/299 [00:00<00:00, 621378.37it/s]\n",
      " 11%|█         | 20/187 [00:00<00:00, 716975.04it/s]\n",
      "  6%|▌         | 20/338 [00:00<00:00, 710898.98it/s]\n",
      "  6%|▌         | 20/328 [00:00<00:00, 570653.61it/s]\n",
      "  7%|▋         | 20/285 [00:00<00:00, 607870.14it/s]\n",
      "  7%|▋         | 20/295 [00:00<00:00, 693273.39it/s]\n",
      "  4%|▎         | 20/554 [00:00<00:00, 671088.64it/s]\n",
      " 10%|▉         | 20/203 [00:00<00:00, 626015.52it/s]\n",
      "  7%|▋         | 20/277 [00:00<00:00, 735842.81it/s]\n",
      "  5%|▍         | 20/427 [00:00<00:00, 555536.95it/s]\n",
      " 11%|█         | 20/179 [00:00<00:00, 541200.52it/s]\n",
      " 10%|▉         | 20/203 [00:00<00:00, 541200.52it/s]\n",
      "  8%|▊         | 20/264 [00:00<00:00, 603496.98it/s]\n",
      "  5%|▍         | 20/413 [00:00<00:00, 716975.04it/s]\n",
      "  5%|▍         | 20/439 [00:00<00:00, 716975.04it/s]\n",
      " 13%|█▎        | 20/150 [00:00<00:00, 607870.14it/s]\n",
      " 10%|█         | 20/191 [00:00<00:00, 671088.64it/s]\n",
      " 16%|█▌        | 20/128 [00:00<00:00, 586615.94it/s]\n",
      " 13%|█▎        | 20/150 [00:00<00:00, 603496.98it/s]\n",
      "  8%|▊         | 20/264 [00:00<00:00, 570653.61it/s]\n",
      "  4%|▍         | 20/503 [00:00<00:00, 665762.54it/s]\n",
      "/var/folders/y3/l8bvkgx53cd0nbrk2b450t7m0000gn/T/ipykernel_10937/632893083.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['krword_rank'] = df['review'].apply(krword)\n"
     ]
    }
   ],
   "source": [
    "df['krword_rank'] = df['review'].apply(krword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/bigcon/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  5%|▍         | 8/169 [00:30<10:06,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 18/169 [01:12<09:42,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 28/169 [01:53<08:59,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 38/169 [02:34<08:25,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 48/169 [03:15<07:50,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 58/169 [04:21<09:25,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 68/169 [05:03<06:36,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 78/169 [05:45<05:58,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 82/169 [06:04<06:16,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 87/169 [06:26<05:33,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 88/169 [06:32<06:35,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 96/169 [07:07<04:59,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 106/169 [07:50<04:14,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 116/169 [08:32<03:27,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 126/169 [09:14<02:48,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 136/169 [09:55<02:08,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 146/169 [10:37<01:30,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 156/169 [11:18<00:50,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 166/169 [12:02<00:11,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170번째 키워드 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [12:16<00:00,  4.36s/it]\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import time\n",
    "api_key = ###\n",
    "genai.configure(api_key = api_key)\n",
    "mct_keyword = {}\n",
    "llm = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "num = 1\n",
    "for place, pos_tag in tqdm(df[['MCT_NM','pos_tag']].values):\n",
    "\n",
    "        context = pos_tag\n",
    "    \n",
    "                        \n",
    "        question = \"\"\"\n",
    "        MBTI 별 맛집 성향에 따른 음식점 추천을 하려고 해. 주어진 제주도 맛집에 대한 리뷰 명사 및 동사 텍스트 기반해 주요 20개의 키워드를 추출해줘. 설명 없이 키워드만 나열해줘.\n",
    "        키워드를 통해 맛, 만족도, 서비스, 분위기, 음식량 등을 파악할 수 있도록 추출해줘.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        messages = f\"\"\"\n",
    "            주어진 리뷰 명사 및 동사 텍스트를 기반으로 사용자의 질문에 답변하세요.\n",
    "            답변은 한국어로 작성하세요.\n",
    "\n",
    "            리뷰 명사 및 동사 텍스트: {context}\n",
    "\n",
    "            사용자 질문: {question}\n",
    "            \n",
    "            답변 예시 : 음식점1 : [키워드1, 키워드2, 키워드3, 키워드4, 키워드5, 키워드6, 키워드7, 키워드8, 키워드9, 키워드10, 키워드11, 키워드12, 키워드13, 키워드14, 키워드15, 키워드16, 키워드17, 키워드18, 키워드19, 키워드20]\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "        try:\n",
    "                response = llm.generate_content(\n",
    "                                messages,\n",
    "                                #safety_settings=\"BLOCK_NONE\",\n",
    "                        \n",
    "                        )\n",
    "                time.sleep(3)\n",
    "                mct_keyword[place] = response.text\n",
    "                num += 1\n",
    "        except Exception as e:\n",
    "                time.sleep(5)\n",
    "                print(e)\n",
    "                num += 1\n",
    "                llm = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "                response = llm.generate_content(\n",
    "                                messages,\n",
    "                                #safety_settings=\"BLOCK_NONE\",\n",
    "                        \n",
    "                        )\n",
    "                if response.text != '':\n",
    "                        mct_keyword[place] = response.text\n",
    "                        num += 1\n",
    "                else:\n",
    "                        pass\n",
    "\n",
    "        if num % 10 == 0:\n",
    "                # print(f'{num}번째 키워드 추출 완료')\n",
    "                time.sleep(3)\n",
    "                llm = genai.GenerativeModel(\"gemini-1.5-flash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_df = pd.DataFrame(mct_keyword.items(), columns=['MCT_NM', 'LLM_keywords'])\n",
    "pd.merge(df, llm_df, on='MCT_NM', how='left').to_csv('review_keyword.csv', index=False)\n",
    "review_df = pd.read_csv('review_keyword.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
