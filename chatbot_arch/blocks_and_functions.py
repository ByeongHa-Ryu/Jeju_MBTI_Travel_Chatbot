from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langchain.agents.agent_types import AgentType
from langchain_core.output_parsers import StrOutputParser,JsonOutputParser
from langchain.memory import ConversationTokenBufferMemory
from langchain_google_genai import GoogleGenerativeAI # type: ignore
from langchain.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.runnables import ConfigurableField

from dotenv import load_dotenv
import os
import pandas as pd
from chatbot_arch.prompts import * 
import streamlit as st
from transformers import AutoTokenizer, AutoModel
import numpy as np 

import folium
from folium import plugins
from streamlit_folium import st_folium
from geopy.distance import geodesic


"""LLM Blocks"""

def mbti_month(mbti, month):
    mbti = mbti
    month = month

load_dotenv()
my_api_key = os.getenv("GOOGLE_API_KEY")

## LLM call 
llm = GoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=my_api_key)

## dataframe agent 
data_dir = './chatbot_arch/data/JEJU_MCT_DATA_final.csv'
df = pd.read_csv(data_dir)

## Data Frame agent on Gemini Engine 
agent = create_pandas_dataframe_agent(
    llm=llm,                           
    df=df,                             
    verbose=True,                      
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, 
    output_parser=StrOutputParser(),
    prompt=agent_inst,
    allow_dangerous_code=True 
)

def validate_mbti(mbti):
    if len(mbti) != 4:
        return False
    
    valid_pairs = {
        0: {"E", "I"},
        1: {"N", "S"},
        2: {"F", "T"},
        3: {"P", "J"}
    }
    
    return all(mbti[i] in valid_pairs[i] for i in range(4))

def clear_chat_history():
    st.session_state.memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=3000)
    st.session_state.messages = [{"role": "assistant", "content": "ì œì£¼ë„ë¥¼ ì—¬í–‰í•˜ê¸° ë”± ì¢‹ì€ ë‚ ì”¨ë„¤ìš” ğŸ˜"}]
    st.session_state.all_restaurants = pd.DataFrame()  # ë§›ì§‘ ë°ì´í„° ì´ˆê¸°í™”
    st.session_state.all_tourist_spots = []  # ê´€ê´‘ì§€ ë°ì´í„° ì´ˆê¸°í™”
    st.session_state.map_center = [33.384, 126.551]  # ì§€ë„ ì¤‘ì‹¬ ì¢Œí‘œ ì´ˆê¸°í™”
    st.session_state.radius = 50  # ë°˜ê²½ ì´ˆê¸°í™” (ì œì£¼ë„ ì „ì²´ ë°˜ê²½)


def load_df(mbti, month):
    model_name = "upskyy/bge-m3-Korean"
    model_kwargs = {'device': 'cpu'}
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs
    )

    loaded_db = FAISS.load_local(
        folder_path=f"./chatbot_arch/database/mct_db/{mbti}/{month}",
        index_name=f"mct_{mbti}_{month}",
        embeddings=embeddings,
        allow_dangerous_deserialization=True,
    )

    return loaded_db

def load_tour_df(mbti, month):
    model_name = "upskyy/bge-m3-Korean"
    model_kwargs = {'device': 'cpu'}
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs
    )

    loaded_tour_db = FAISS.load_local(
        folder_path=f"./chatbot_arch/database/tour_db/{mbti}/{month}",
        index_name=f"tour_{mbti}_{month}",
        embeddings=embeddings,
        allow_dangerous_deserialization=True,
    )

    return loaded_tour_db


def retrieve_mct(loaded_db, query):
   retriever = loaded_db.as_retriever(
       search_type="mmr", 
       search_kwargs={
           "k": 3,
           "fetch_k": 20,
           "lambda_mult": 0.8
       }
   )
   query = f"{query} {st.session_state.get('mbti', '')}"
   return retriever.get_relevant_documents(query)

def load_faiss_and_search(query, mbti, month, k=3,):
    try:
        loaded_db = load_df(mbti, month)
        retrieve_documents = retrieve_mct(loaded_db, query)

        search_results = []
        for doc in retrieve_documents:
            metadata = doc.metadata
            contents = doc.page_content
            try:
                search_results.append({
                    'index': metadata['idx'],
                    'ì •ë³´' : contents,
                    'ìœ„ë„': float(metadata['ìœ„ë„']) if pd.notna(metadata['ìœ„ë„']) else np.nan,
                    'ê²½ë„': float(metadata['ê²½ë„']) if pd.notna(metadata['ê²½ë„']) else np.nan,
                })
            except (ValueError, TypeError):
                search_results.append({
                    'index': metadata['idx'],
                    'ì •ë³´' : contents,
                    'ìœ„ë„': np.nan,
                    'ê²½ë„': np.nan,
                })
        return search_results
        
    except Exception as e:
        st.error(f"FAISS ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def get_restaurant_details(search_results, restaurants_df, mbti, month):
    try:
        restaurants_data = []
        restaurant_db = load_df(mbti, month)
        all_docs = restaurant_db.docstore._dict

        for result in search_results:
            # Fixed: Added idx key access
            if 'index' in result: # ì¬ë°©ë¬¸ë¥ 
                restaurant = restaurants_df.iloc[result['index']]
                restaurant_info = {
                'ìŒì‹ì ëª…': restaurant['ê°€ë§¹ì ëª…'],
                'ì£¼ì†Œ': restaurant['ì£¼ì†Œ'],
                'ì—…ì¢…': restaurant['ì—…ì¢…'],
                'ì •ë³´': result['ì •ë³´'],
                'ìœ„ë„': result['ìœ„ë„'],
                'ê²½ë„': result['ê²½ë„'],
                }

            for doc in all_docs.values():
                    if doc.metadata.get('idx') == result['index']:
                        restaurant_info['ìƒì„¸ì„¤ëª…'] = doc.page_content
                        break
                        
            restaurants_data.append(restaurant_info)
                
        return pd.DataFrame(restaurants_data)

    except Exception as e:
        st.error(f"ìŒì‹ì  ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def find_nearby_tourist_spots(restaurant_row, tourist_spots_df, mbti, month, n=3):
    try:
        if pd.isna(restaurant_row['ìœ„ë„']) or pd.isna(restaurant_row['ê²½ë„']):
            return []

        restaurant_coords = (restaurant_row['ìœ„ë„'], restaurant_row['ê²½ë„'])
        distances = []
        matching_docs = []
        for i, tourist in tourist_spots_df.iterrows():
            if pd.isna(tourist['ìœ„ë„']) or pd.isna(tourist['ê²½ë„']):
                continue
            try:
                tourist_coords = (tourist['ìœ„ë„'], tourist['ê²½ë„'])
                distance = geodesic(restaurant_coords, tourist_coords).kilometers
                
                distances.append({
                    'ê´€ê´‘ì§€ëª…': tourist['ê´€ê´‘ì§€ëª…'],
                    'ì£¼ì†Œ': tourist['ì£¼ì†Œ'],
                    'ìœ„ë„': tourist['ìœ„ë„'],
                    'ê²½ë„': tourist['ê²½ë„'],
                    'ê±°ë¦¬': distance,
                    'idx': i
                })

            except Exception as e:
                print(f"ê±°ë¦¬ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ê´€ê´‘ì§€: {tourist['ê´€ê´‘ì§€ëª…']}): {str(e)}")
                continue
        
        sorting = sorted(distances, key=lambda x: x['ê±°ë¦¬'])[:n] if distances else []

        tourist_db = load_tour_df(mbti, month)
        all_docs = tourist_db.docstore._dict
        
        result = []
        for spot in sorting:
            spot_info = spot.copy()  # ê¸°ì¡´ ê±°ë¦¬ ì •ë³´ ë³µì‚¬
            
            # í•´ë‹¹ ê´€ê´‘ì§€ì˜ ë¬¸ì„œ ì •ë³´ ì°¾ê¸°
            for doc in all_docs.values():
                if doc.metadata.get('idx') == spot['idx']:
                    spot_info['ìƒì„¸ì„¤ëª…'] = doc.page_content  # matching_docs ì •ë³´ ì¶”ê°€
                    result.append(spot_info)
                    break
        
        return result
        
    except Exception as e:
        st.error(f"ê´€ê´‘ì§€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

def create_map_with_restaurants_and_tourists(restaurants_df, tourist_spots):
    try:
        if restaurants_df is None or restaurants_df.empty:
            st.warning("ë§›ì§‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        valid_locations = restaurants_df[restaurants_df['ìœ„ë„'].notna() & restaurants_df['ê²½ë„'].notna()]
        if valid_locations.empty:
            st.warning("ìœ íš¨í•œ ìœ„ì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        center_lat = valid_locations.iloc[0]['ìœ„ë„']
        center_lng = valid_locations.iloc[0]['ê²½ë„']
        
        m = folium.Map(
            location=[center_lat, center_lng],
            zoom_start=14,
            tiles='CartoDB positron',
            width=800,
            height=600
        )
        
        for idx, row in restaurants_df.iterrows():
            if pd.isna(row['ìœ„ë„']) or pd.isna(row['ê²½ë„']):
                continue
                
            popup_content = f"""
                <div style='width: 200px'>
                    <h4 style='margin: 0; padding: 5px 0;'>{row['ìŒì‹ì ëª…']}</h4>
                    <p style='margin: 5px 0;'>ì£¼ì†Œ: {row['ì£¼ì†Œ']}</p>
                </div>
            """
            
            folium.Marker(
                location=[row['ìœ„ë„'], row['ê²½ë„']],
                popup=folium.Popup(popup_content, max_width=300),
                icon=folium.Icon(color='red', icon='info-sign')
            ).add_to(m)
            
            if idx < len(tourist_spots):
                for tourist in tourist_spots[idx]:
                    if pd.isna(tourist['ìœ„ë„']) or pd.isna(tourist['ê²½ë„']):
                        continue
                        
                    tourist_popup = f"""
                        <div style='width: 200px'>
                            <h4 style='margin: 0; padding: 5px 0;'>{tourist['ê´€ê´‘ì§€ëª…']}</h4>
                            <p style='margin: 5px 0;'>ì£¼ì†Œ: {tourist['ì£¼ì†Œ']}</p>
                        </div>
                    """
                    
                    folium.Marker(
                        location=[tourist['ìœ„ë„'], tourist['ê²½ë„']],
                        popup=folium.Popup(tourist_popup, max_width=300),
                        icon=folium.Icon(color='blue', icon='info-sign')
                    ).add_to(m)
        
        return m
        
    except Exception as e:
        st.error(f"ì§€ë„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def display_map_with_data(map_obj):
    try:
        if map_obj:
            if 'map_container' not in st.session_state:
                st.session_state.map_container = st.empty()
                
            with st.session_state.map_container:
                st_folium(
                    map_obj,
                    width=700,
                    height=500,
                    key="persistent_map",
                    returned_objects=[]
                )
                
    except Exception as e:
        st.error(f"ì§€ë„ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@st.cache_data
def create_cached_map(_restaurants_data, _tourist_spots):
    return create_map_with_restaurants_and_tourists(_restaurants_data, _tourist_spots)

def format_restaurant_and_tourist_response(restaurants_df, tourist_spots):
    try:
        if restaurants_df is None or restaurants_df.empty:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë§›ì§‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        formatted_data = []
        for idx, row in restaurants_df.iterrows():
            restaurant_info = {
                "ë§›ì§‘ ì„¤ëª…": row['ìƒì„¸ì„¤ëª…'],
                "ì£¼ë³€_ê´€ê´‘ì§€": [
                    {
                        "ê´€ê´‘ì§€ ì„¤ëª…": tourist['ìƒì„¸ì„¤ëª…'],
                        "ê±°ë¦¬": f"{tourist['ê±°ë¦¬']:.1f}km"
                    }
                    for tourist in tourist_spots[idx]
                ]
            }
            formatted_data.append(restaurant_info)
        print(formatted_data)
        return formatted_data
        
    except Exception as e:
        return f"ì •ë³´ í¬ë§¤íŒ… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def generate_llm_response(query, formatted_data, user_mbti=None):
    try:
        response = llm.invoke(
            input=recommend_inst.format(
            query=query, 
            formatted_data=formatted_data, 
            user_mbti=user_mbti)
        )   
        return response
        
    except Exception as e:
        return f"LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# ì±—ë´‡ ì½”ë“œ ìˆ˜ì •
def process_recommendation(message, mbti, month):
    try:
        restaurants_df = pd.read_csv(f"./chatbot_arch/data/mct_df/{mbti}/mct_{month}_{mbti}.csv")
        tourist_spots_df = pd.read_csv(f"./chatbot_arch/data/tour_df/{mbti}/tour_{month}_{mbti}.csv")
        
        search_results = load_faiss_and_search(message, mbti, month, k=3)
        if not search_results:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹í•˜ëŠ” ë§›ì§‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", None, None
            
        restaurants_data = get_restaurant_details(search_results, restaurants_df, mbti, month)
        if restaurants_data is None:
            return "ìŒì‹ì  ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", None, None
            
        tourist_spots = []
        for _, restaurant in restaurants_data.iterrows():
            nearby_spots = find_nearby_tourist_spots(restaurant, tourist_spots_df, mbti, month)
            tourist_spots.append(nearby_spots)  

        formatted_data = format_restaurant_and_tourist_response(restaurants_data, tourist_spots)
        user_mbti = st.session_state.get('mbti', None)
        response = generate_llm_response(message, formatted_data, user_mbti)

        # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ëˆ„ì  ì €ì¥
        # ì¤‘ë³µì œê±°
        if 'all_restaurants' not in st.session_state:
            mask = restaurants_data['ìŒì‹ì ëª…'].apply(lambda x: x in str(response))
            filtered_restaurants = restaurants_data[mask]
            st.session_state.all_restaurants = filtered_restaurants

        else:
            mask = restaurants_data['ìŒì‹ì ëª…'].apply(lambda x: x in str(response))
            filtered_restaurants = restaurants_data[mask]

            # í•„í„°ë§ëœ ë°ì´í„°ë¥¼ ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
            st.session_state.all_restaurants = pd.concat(
                [st.session_state.all_restaurants, filtered_restaurants],
                ignore_index=True
            ).drop_duplicates(subset=['ìŒì‹ì ëª…'])

        tourist_spots_lst = []
        for i in range(3):
            k = tourist_spots[i]
            for j in range(3):
                tourist_spots_lst.append(k[j])
    
        if 'all_tourist_spots' not in st.session_state:
            tourist_spots_filter = [item for item in tourist_spots_lst if item['ê´€ê´‘ì§€ëª…'] in str(response)]
            deduplicated_dict = {}
            for item in tourist_spots_filter:
                tourist_spot = item['ê´€ê´‘ì§€ëª…']
                # ê°™ì€ ê´€ê´‘ì§€ëª…ì´ ìˆì„ ê²½ìš° ë§ˆì§€ë§‰ ë°ì´í„°ë¡œ ë®ì–´ì”Œì›Œì§
                deduplicated_dict[tourist_spot] = item
            
            # ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            st.session_state.all_tourist_spots = list(deduplicated_dict.values())

        else:
            # ë‘ ë°ì´í„°ë¥¼ í•©ì¹˜ê¸°
            merged_data = st.session_state.all_tourist_spots + tourist_spots_lst
            
            # ê´€ê´‘ì§€ëª…ì„ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
            # ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ ì¤‘ë³µì´ ì œê±°ë¨
            deduplicated_dict = {}
            for item in merged_data:
                tourist_spot = item['ê´€ê´‘ì§€ëª…']
                # ê°™ì€ ê´€ê´‘ì§€ëª…ì´ ìˆì„ ê²½ìš° ë§ˆì§€ë§‰ ë°ì´í„°ë¡œ ë®ì–´ì”Œì›Œì§
                deduplicated_dict[tourist_spot] = item
            
            # ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            st.session_state.all_tourist_spots = list(deduplicated_dict.values())

        return response, restaurants_data, tourist_spots
        
    except Exception as e:
        print(f"Error in recommendation process: {e}")
        return f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", None, None