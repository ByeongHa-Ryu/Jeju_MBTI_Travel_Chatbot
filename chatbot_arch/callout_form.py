import os
from contextlib import redirect_stdout
import io
from chatbot_arch.prompts import *
# from blocks_and_functions import *
import streamlit as st
from streamlit_folium import folium_static
import google.generativeai as genai
from langchain.vectorstores import FAISS
from streamlit_folium import st_folium
import os
from contextlib import redirect_stdout
import io
from chatbot_arch.blocks_and_functions import *



## user_mbti
def Callout(message, memory, user_mbti, month):
    try:
        # Query classification 
        classification_response = llm.invoke(input=cls_llm_inst.format(input_query=message,memory = memory,few_shot_prompt_for_cls=few_shot_prompt_for_cls))
        if "ë¶„ì„ ê´€ë ¨" in classification_response:
            
            f = io.StringIO()
            with redirect_stdout(f):
                analysis_result = agent.invoke(
                    input=agent_inst.format(
                        input_query = message,
                        data_info_prompt = data_info_prompt
                    ))
                
            verbose_output = f.getvalue()

            final_response = llm.invoke(
                input=post_agent_inst.format(
                    input_query = message,
                    verbose_output = verbose_output,
                    analysis_result = analysis_result
                    )
            )
            write_log('********************************\n')
            write_log(f'query : {message} \n')
            write_log("ë¶„ì„ê´€ë ¨\n")
            write_log(f"analysis_result :  {analysis_result} \n")
            write_log(f"final_response : {final_response} \n")

        ### ì—¬ê¸° ë³€ê²½

        elif "ì¶”ì²œ ê´€ë ¨" in classification_response:
            try:
                final_response, restaurants_data, tourist_spots = process_recommendation(message, user_mbti, month)
                # Process recommendation                
                if restaurants_data is not None and tourist_spots is not None:
                    # Create containers if they don't exist
                    if 'title_container' not in st.session_state:
                        st.session_state.title_container = st.container()
                    if 'info_section' not in st.session_state:
                        st.session_state.info_section = st.container()
                    
                    # Display title
                    with st.session_state.title_container:
                        st.subheader("ğŸ—ºï¸ ì¶”ì²œ ë§›ì§‘ê³¼ ì£¼ë³€ ê´€ê´‘ì§€")
                
                write_log('******************************** \n')
                write_log('query : ' + message+' \n')
                write_log("ì¶”ì²œê´€ë ¨ \n")
                write_log(f"final_response : {final_response} \n")
                write_log(f"restaurants_data : {restaurants_data} \n")
                write_log(f"tourist_spots : {tourist_spots} \n")

                return final_response
                
            except Exception as e:
                return f"ë§›ì§‘ ì¶”ì²œ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            
        elif "ë¶€ê°€ ì§ˆë¬¸" in classification_response:
            restaurants_state, tourist_spots_state = state()
            print('ë¶€ê°€ì§ˆë¬¸:         ', restaurants_state)
            print('ë¶€ê°€ì§ˆë¬¸:         ', tourist_spots_state)
            final_response = llm.invoke(
                input=question_inst.format(
                    input_query = message,
                    memory = memory,
                    restaurants = restaurants_state,
                    tour = tourist_spots_state
                    )
            )
            write_log('********************************\n')
            write_log(f'query : {message} \n')
            write_log("ë¶€ê°€ì§ˆë¬¸"+' \n')
            write_log(f"prompt: {question_inst}")
            write_log(f"final_response : {final_response} \n")
            write_log(f"memory : {memory} \n")
            write_log(f"restaurants : {restaurants} \n")
            write_log(f"tour : {tour} \n")
            
    
        else:
            final_response = llm.invoke(
                input=main_persona.format(input_query=message,memory = memory)
            )

            write_log('********************************\n')
            write_log(f'query : {message} \n')
            write_log("ì¼ë°˜ì§ˆë¬¸"+' \n')
            write_log(f"final_response : {final_response} \n")

    except Exception as e:
        final_response = f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    return final_response